\documentclass[11pt,twoside,a4paper]{article}

%% BibTeX settings
\usepackage[authoryear,round]{natbib}

%% additional packages
\usepackage[latin1]{inputenc}
\usepackage{a4wide,graphicx,color,thumbpdf}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{url}

% hyperref setup
\definecolor{Red}{rgb}{0.5,0,0}
\definecolor{Blue}{rgb}{0,0,0.5}
\hypersetup{%
  pdftitle = {Lesson 6 - Raster Analysis},
  pdfsubject = {},
  pdfkeywords = {Landsat, time series},
  pdfauthor = {Ben DeVries},
  %% change colorlinks to false for pretty printing
  colorlinks = {true},
  linkcolor = {Blue},
  citecolor = {Blue},
  urlcolor = {Red},
  hyperindex = {true},
  linktocpage = {true},
}

\usepackage{Sweave} %% is essentially

\begin{document}
\SweaveOpts{concordance=TRUE}

\title{Raster Analysis}
\author{Ben DeVries, Jan Verbesselt, Loic Dutrieux, Sytze de Bruin}

\maketitle

\begin{abstract}

In this tutorial, we will explore the raster package and other related packages used for typical raster analyses. We will first look at analysis of RasterLayer objects, exploring functions having to do with raster algebra, focal and zonal statistics and other operations. We will then explore spatio-temporal analysis of raster data using RasterBrick objects. Here, we will extract time series data from a RasterBrick object and derive temporal statistics from these data.
\begin{enumerate}
  \item perform typical image preprocessing operations (apply a mask, calculate ndvi, etc.)
  \item overlaying with geodata and calculating zonal statistics
  \item explore a raster brick by plotting layers and layers statistics
  \item perform raster brick operations to derive stats (e.g. \%no-data in the time series)
  \item extract pixel time series and derive various statistics
\end{enumerate}
\end{abstract}

\section{The raster package}

The raster package is an essential tool for raster-based analysis in R. Here you will find functions which are fundamental to image analysis. The raster package documentation is a good place to begin exploring the possibilities of image analysis within R. There is also an excellent vignette available at \url{http://cran.r-project.org/web/packages/raster/vignettes/Raster.pdf}.

\section{The Landsat archive}

Since being released to the public, the Landsat data archive has become an invaluable tool for environmental monitoring. With a historical archive reaching back to the 1970's, the release of these data has resulted in a spur of time series based methods. In this tutorial, we will work with time series data from the Landsat 7 Enhanced Thematic Mapper (ETM+) sensor.

\section{Manipulating raster data}

This section will cover some of the following areas:
\begin{itemize}
  \item calculate \% of no data in a raster
  \item crop a raster based on a defined extent, another object's extent, or interactively using drawExtent()
  \item working with the calc() and overlay() functions:
  \begin{itemize}
    \item load and apply a mask (cloud mask)
    \item raster algebra with two or more raster layers
  \end{itemize}
  \item focal operations; create a function to 'sieve' a raster object (remove lone pixels)
  \item create an areal filter for a raster by converting to a SpatialPolygon, removing small features and back to raster using the rasterize() function
  \item zonal statistics using other geodata (Biosphere Reserve zones, administrative zones, etc...)
  \item reclassifying (stratifying) a raster based on elevation data (SRTM or ASTER2)
\end{itemize}
  
\section{Working with multilayered raster data}

When working with multispectral or multitemporal raster data, it is convenient to represent multiple raster layers as a single object in R. R works with two types of multilayer raster objects: stacks and bricks. The main difference between the two is that raster stacks can be read from several different data sources (files) and bricks are read from a single file (e.g. a multiband GeoTIFF).

A raster brick from a small area within the Kafa Biosphere Reserve in Southern Ethiopia can be found in the rasta package. Set the working directory to the packages home folder and load the raster brick from file by

% this chunk runs silently
<<echo = FALSE, results = hide>>=
library(rasta)
f <- system.file("extdata/tura.grd", package="rasta")
# tura <- brick('~/R/projects/rasta/pkg/rasta/inst/extdata/tura.grd')
tura <- brick(f) ## Jan's comment: the package needs to be compiled first including the tura.gr* files.
@
% the following is just for show
\begin{Sinput}
> setwd('path/to/rasta/package') # set this to the appropriate directory
> tura <- brick('inst/extdata/tura.grd')
\end{Sinput}
<<>>=
# inspect the data
class(tura) # the object's class
projection(tura) # the projection
res(tura) # the spatial resolution (x, y)
extent(tura) # the extent of the raster brick
@

\subsection*{Extracting scene information}
This RasterBrick was read from a .grd file. One advantage of this file format (over the GeoTIFF format, for example) is the fact that the specific names of the raster layers making up this brick have been preserved, a feature which is important for identifying raster layers, especially when doing time series analysis (where you need to know the values on the time axis). This RasterBrick was prepared from a Landsat 7 ETM+ time series, and the original scene names were inserted as layer names.

<<results = hide>>=
names(tura) # displays the names of all layers in the tura RasterBrick
@

We can parse these names to extract information from them. The first 3 characters indicate which sensor the data come from, with 'LE7' indicating Landsat 7 ETM+ and 'LT5' or 'LT4' indicating Landsat 5 and Landsat 4 TM, respectively. The following 6 characters indicate the path and row (3 digits each), according to the WGS system. The following 7 digits represent the date. The date is formatted in such a way that it equals the year + the julian day. For example, February 5th 2001, aka the 36th day of 2001, would be '2001036'.

<<results = hide>>=
# display the 1st 3 characters of the layer names
sensor <- substr(names(tura), 1, 3)
print(sensor)

# display the path and row as numeric vectors in the form (path,row)
path <- as.numeric(substr(names(tura), 4, 6))
row <- as.numeric(substr(names(tura), 7, 9))
print(paste(path, row, sep = ","))

# display the date
dates <- substr(names(tura), 10, 16)
print(dates)

# format the date in the format yyyy-mm-dd
as.Date(dates, format = "%Y%j")
@

There is a function in the rasta package, getSceneinfo() that will parse these names and output a data.frame with all of these attributes.
% the following code chunk can be deleted once the revised rasta package (with getSceneinfo()) is built and loaded
<<echo = FALSE, results = hide>>=
getSceneinfo <- function(sourcefile, filename="")
  # returns a data.frame with sensor, path, row, and date information for each scene
  # writes to a .csv if a filename/path is specified
  
  # args:
    # sourcefile - a character vector of file (*.tar.gz or *.tar), folder names or scenenames (in standard Landsat format)
    # filename - optional: write results to .csv file
{
  # for the sake of convenience, sourcefile can be either a character vector of scene names (or subfolders) or the original .tar.gz or .tar files
  # this will check which it is and format accordingly
  sourcefile <- sapply(sourcefile, FUN=function(x){if(grepl(".gz", x)) x <- substr(x, 1, nchar(x)-3);
                                                   if(grepl(".tar", x)) x <- substr(x, 1, nchar(x)-4); 
                                                   return(x)})  
  
  # dates in LS naming system are formatted with year and julian day as one number - "%Y%j" (e.g. 2001036 = 2001-02-05)
  # reformat date as "%Y-%m-%d" (ie. yyyy-mm-dd)
  dates <- as.Date(substr(sourcefile, 10, 16), format="%Y%j")
  
  # identify the sensor
  sensor <- as.character(mapply(substr(sourcefile, 1, 3), dates, FUN=function(x,y){
    if(x == "LE7" & y <= "2003-03-31")
      "ETM+ SLC-on"
    else if(x == "LE7" & y > "2003-03-31")
      "ETM+ SLC-off"
    else if(x == "LT5" | x == "LT4") 
      "TM" 
    else if(x == "LC8")
      "OLI"
    else
      stop(sourcefile, " is not a recognized Landsat5/7/8 scene ID.")
  }))
  # extract path, row
  path <- as.numeric(substr(sourcefile, 4, 6))
  row <- as.numeric(substr(sourcefile, 7, 9))
  
  # throw all attributes into a data.frame
  info <- data.frame(sensor = sensor, path = path, row = row, date = dates)
  row.names(info) <- sourcefile
  
  # optional: print to .csv for future reference
  if(filename!="") 
    write.csv(info, file=filename, quote=FALSE)
  
  return(info)
}
@

<<results = hide>>=
sceneinfo <- getSceneinfo(names(tura))
print(sceneinfo)
# add a 'year' column to the sceneinfo dataframe and plot #scenes/year
sceneinfo$year <- factor(substr(sceneinfo$date, 1, 4), levels = c(1999:2013))
@
\begin{center}
<<fig = TRUE>>=
# barplot with number of scenes per year
library(ggplot2)
ggplot(data = sceneinfo, aes(x = year, fill = sensor)) + 
  geom_bar() + 
  labs(y = "number of scenes") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45))
@
\end{center}

\subsection*{Plotting RasterBricks}
A RasterBrick can be plotted just as a RasterLayer, and the graphics device will automatically split into panels to accommodate the layers (to an extent: R will not attempt to plot 100 layers at once!). To plot the first 9 layers:
\begin{center}
<<fig = TRUE>>=
plot(tura, c(1:9))
# alternatively, you can use [[]] notation to specify layers
plot(tura[[1:9]])
# use the information from sceneinfo data.frame to clean up the titles
plot(tura[[1:9]], main = sceneinfo$date[c(1:9)])
@
\end{center}

Unfortunately, the scale is different for each of the layers, making it impossible to make any meaningful comparison between the raster layers. This problem can be solved by specifying a breaks argument in the plot() function.

<<>>=
# we need to define the breaks to harmonize the scales (to make the plots comparable)
bks <- seq(0, 10000, by = 2000) # (arbitrarily) define the breaks
# we also need to redefine the colour palette to match the breaks
cols <- rev(terrain.colors(length(bks))) # col = rev(terrain.colors(255)) is the default colour palette parameter set in raster plots, but there are many more options available!
# (opt: check out the RColorBrewer package for other colour palettes)
# plot again with the new parameters
plot(tura[[1:9]], main = sceneinfo$date[1:9], breaks = bks, col = cols)
@

Alternatively, the rasterVis package has some enhanced plotting functionality for raster objects, including the levelplot() function, which automatically provides a common scale for the layers.

<<>>=
library(rasterVis)
levelplot(tura[[1:6]])
# NOTE:
# for rasterVis plots we must use the [[]] notation for extracting layers
@
<<>>=
# providing titles to the layers is done using the 'names.attr' argument in place of 'main'
levelplot(tura[[1:6]], names.attr = sceneinfo$date[1:6])
@

This plot gives us a common scale which allows us to compare values (and perhaps detect trends) from layer to layer. In the above plot, the layer titles do not look very nice -- we will solve that problem a bit later.

The rasterVis package has integrated plot types from other packages with the raster package to allow for enhanced analysis of raster data.
<<results = hide>>=
# histograms of the first 6 layers
histogram(tura[[1:6]])
# box and whisker plot of the first 9 layers
bwplot(tura[[1:9]])
@

More examples from the rasterVis package can be found @ \url{http://oscarperpinan.github.io/rastervis/}

\subsection*{Calculating data loss}
In this RasterBrick, the layers have all been individually preprocessed from the raw data format into NDVI values. Part of this process was to remove all pixels obscured by clouds or SLC-off gaps (for any ETM+ data acquired after March 2003). For this reason, it may be useful to know how much of the data has been lost to cloud cover and SLC gaps. First, we will calculate the percentage of no-data pixels in each of the layers using the freq() function. freq() returns a table (matrix) of counts for each value in the raster layer. It may be easer to represent this as a data.frame to access column values.

<<>>=
# try for one layer first
y <- freq(tura[[1]]) # this is a matrix
y <- as.data.frame(y)
# how many NA's are there in this table?
y$count[is.na(y$value)]
# alternatively, using the with() function:
with(y, count[is.na(value)])
# as a %
with(y, count[is.na(value)]) / ncell(tura[[1]]) * 100
# apply this over all layers in the RasterBrick
# first, prepare a numeric vector to be 'filled' in
nas <- vector(mode = 'numeric', length = nlayers(tura))
for(i in 1:nlayers(tura)){
  y <- as.data.frame(freq(tura[[i]]))
  nas[i] <- with(y, count[is.na(value)]) / ncell(tura[[i]]) * 100
}
# add this vector as a column in the sceneinfo data.frame
sceneinfo$nodata <- nas
@
\begin{center}
<<fig = TRUE>>=
# plot these values
ggplot(data = sceneinfo, aes(x = date, y = nodata, shape = sensor)) +
  geom_point(size = 2) +
  labs(y = "% nodata") +
  theme_bw()
@
\end{center}

We have now derived some highly valuable information about our time series. For example, we may want to select an image from our time series with relatively little cloud cover to perform a classification. For further time series analysis, the layers with 100\% data loss will be of no use to us, so it may make sense to get rid of these layers.

<<>>=
# which layers have 100% data loss?
which(sceneinfo$nodata == 100)
# supply these indices to the dropLayer() command to get rid of these layers
tura <- dropLayer(tura, which(sceneinfo$nodata == 100))
# redifine our sceneinfo data.frame as well
sceneinfo <- sceneinfo[which(sceneinfo$nodata != 100), ]
# optional: remake the previous ggplots with this new dataframe
@

With some analyses, it may also be desireable to apply a no-data threshold per scene, in which case layer indices would be selected by:
\begin{Sinput}
> which(sceneinfo$nodata > some_threshold)
\end{Sinput}

In some cases, there may be parts of the study area with more significant data loss due to persistant cloud cover or higher incidence of SLC-off gaps. To map the spatial distribution of data loss, we need to calculate the \% of NA in the time series for each \emph{pixel} (ie. looking 'through' the pixel along the time axis). To do this, it is convenient to use the calc() function and supply a special function which will count the number of NA's for each pixel along the time axis, divide it by the total number of data in the pixel time series, and output a percentage. calc() will output a raster with a percentage no-data value for each pixel.

<<label = NAplot, include = FALSE>>=
# calc() will apply a function over each pixel
# in this case, each pixel represents a time series of NDVI values
# e.g. all values of the 53rd pixel in the raster grid:
y <- as.numeric(tura[53])
# how many of these values have been masked (NA)?
length(y[is.na(y)])
# as a %
length(y[is.na(y)]) / length(y) * 100
# now wrap this in a calc() to apply over all pixels of the RasterBrick
nodata <- calc(tura, fun = function(x) length(x[is.na(x)]) / length(x) * 100)
@
\begin{center}
<<fig = TRUE, echo = FALSE>>=
plot(nodata)
@
\end{center}
<<>>=
# block out pixels with 100% data loss (these have already been masked)
nodata[nodata == 100] <- NA
@
\begin{center}
<<fig = TRUE, echo = FALSE>>=
plot(nodata)
@
\end{center}

\section{Times Series Analysis}
\begin{itemize}
\item making time series plots for selected pixels (interactively or otherwise)
\item deriving time series statistics
\begin{itemize}
\item linear regression from pixel time series
\item deriving seasonality parameters?
\item creating figures maps using these statistics
\end{itemize}
\end{itemize}


\section{Exercise}
\begin{itemize}
\item to be doable in 3 hours....
\item combine concepts from previous lessons as well
\item example 1: produce a figure with maximum/minimum/median/mean NDVI per year; figure should have a common scale and be properly labelled
\end{itemize}

\bibliographystyle{model5-names}
\bibliography{refs}

\end{document}
