\documentclass[11pt,twoside,a4paper]{article}

%% BibTeX settings
\usepackage[authoryear,round]{natbib}

%% additional packages
\usepackage[latin1]{inputenc}
\usepackage{a4wide,graphicx,color,thumbpdf}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{url}

% hyperref setup
\definecolor{Red}{rgb}{0.5,0,0}
\definecolor{Blue}{rgb}{0,0,0.5}
\def\code#1{\texttt{#1}}
\hypersetup{%
  pdftitle = {Lesson 6 - Raster Analysis},
  pdfsubject = {},
  pdfkeywords = {Landsat, time series},
  pdfauthor = {Ben DeVries},
  %% change colorlinks to false for pretty printing
  colorlinks = {true},
  linkcolor = {Blue},
  citecolor = {Blue},
  urlcolor = {Red},
  hyperindex = {true},
  linktocpage = {true},
}

\usepackage{Sweave} %% is essentially

\begin{document}
\SweaveOpts{concordance=TRUE}

\title{Raster Analysis}
\author{Ben DeVries, Jan Verbesselt, Lo\"{i}c Dutrieux}

\maketitle

\section{Learning Objectives}
The learning objectives for this lecture are:
\begin{itemize}
\item explore the raster package and related packages used for typical raster analyses
\item learn several approaches to visualizing raster data
\item become acquainted with Landsat data
\item carry out a supervised classification (random forest) on a series of raster layers
\item work with thematic rasters
\item construct a raster sieve using the \code{focal()} function
\end{itemize}

\section{Raster and related packages}

The raster package is an essential tool for raster-based analysis in R. Here you will find functions which are fundamental to image analysis. The raster package documentation is a good place to begin exploring the possibilities of image analysis within R. There is also an excellent vignette available at \url{http://cran.r-project.org/web/packages/raster/vignettes/Raster.pdf}.

In addition to the raster package, we will be using the rasterVis and ggplot2 packages to make enhanced plots.

<<packages>>=
# load the necessary packages
library(rasta)
library(raster)
library(rgdal)
library(sp)
library(ggplot2)
library(rasterVis)
@

\section{The Landsat archive}

Since being released to the public, the Landsat data archive has become an invaluable tool for environmental monitoring. With a historical archive reaching back to the 1970's, the release of these data has resulted in a spur of time series based methods. In this tutorial, we will work with time series data from the Landsat 7 Enhanced Thematic Mapper (ETM+) sensor.

\section{Manipulating raster data}

\subsection*{Exploring a Landsat scene}

Landsat scenes are delivered via the USGS as a number of image layers representing the different bands captured by the sensors. In the case of the Landsat 7 Enhanced Thematic Mapper (ETM+) sensor, the bands are shown in Figure \ref{landsatBands}. Using different combination of these bands can be useful in describing land features and change processes.

\begin{figure}[!htp]
\centering
\includegraphics{figs/landsat_bands.jpg}
  \caption{Bands included in the Landsat 7 (ETM+) and Landsat 8 (OLI/TIRS) sensors. Source: NASA.}
  \label{landsatBands}
\end{figure}

Part of a landsat scene, including bands 2-4 are included as part of the rasta package. These data have been processed using the LEDAPS framework\footnote{http://ledaps.nascom.nasa.gov/}, so the values contained in this dataset represent surface reflectance, scaled by 10000 (ie. divide by 10000 to get a reflectance value between 0 and 1).

We will begin exploring these data simply by visualizing them (more methods for data exploration will be covered in Lesson 7).

<<explore_data, fig=FALSE, results=hide>>=
# load in the data
data(GewataB2)
data(GewataB3)
data(GewataB4)
# check out the attributes
GewataB2
# some basic statistics using cellStats()
cellStats(GewataB2, stat=max)
# ...is equivalent to:
maxValue(GewataB2)
# what is the maximum value of all three bands?
max(c(maxValue(GewataB2), maxValue(GewataB3), maxValue(GewataB4)))

# plot the histograms of all bands
hist(GewataB2)
hist(GewataB3)
hist(GewataB4)
@

We can improve these plots by adjusting the axis scale, bin size, etc., but the rasterVis package that has enhanced plotting capabilities which make it easier to make more attractive plots. First, to make the comparison easier, we will make a rasterBrick object from these three layers.

<<histogram, fig=FALSE>>=
library(rasterVis)
gewata <- brick(GewataB2, GewataB3, GewataB4)
# view all histograms together with rasterVis
histogram(gewata)
@

The rasterVis package has several other raster plotting types inherited from the lattice pacakge. For multispectral data, one plot type which is particularly useful for data exploration is the scatterplot matrix, called by the \code{splom()} function.

<<spplom, fig=TRUE, include=FALSE>>=
splom(gewata)
@
\begin{figure}[!htp]
  \centering
  \includegraphics[width=0.6\textwidth]{Lesson_6-spplom}
  \caption{Scatter plot matrix for Landsat bands 2, 3, and 4 of the 'Gewata'
  sub scene.}
  \label{scatterplotmatrix}
\end{figure}

Calling \code{splom()} on a rasterBrick reveals potential correlations between the layers themselves(Figure \ref{scatterplotmatrix}). In the case of bands 2-4 of the gewata subset, we can see that band 2 and 3 (in the visual part of the EM spectrum) are highly correlated, while band 4 contains significant non-redundant information. Given what we know about the location of these bands along the EM spectrum (Figure \ref{landsatBands}), how could these scatterplots be explained? ETM+ band 4 (nearly equivalent to band 5 in the Landsat 8 OLI sensor) is situated in the near infrared (NIR) region of the EM spectrum and is often used to described vegetation-related features.

The rasterVis package will be demonstrated in more detail in this tutorial the section dealing with raster time series.

\subsection*{Computing new rasters: Raster algebra}

In the previous section, we observed a strong correlation between two of the Landsat bands of the gewata subset, but a very different distribution of values in band 4 (NIR). This distribution stems from the fact that vegetation reflects very highly in the NIR range, compared to the visual range of the EM spectrum. A commonly used metric for assessing vegetation dynamics, the normalized difference vegetation index (NDVI), takes advantage of this fact and is computed from Landsat bands 3 (red; R) and 4 (NIR). In Lesson 5, we explored several ways to calculate NDVI, using raster algebra, \code{calc()} or \code{overlay()}. 

<<ndvi>>=
ndvi <- overlay(GewataB4, GewataB3, fun=function(x,y){(x-y)/(x+y)})
@

One advantage of the \code{overlay()} function is the fact that the result can be written immediately to file by including the argument \code{filename="..."}, thus saving memory (especially important when working with large datasets).

Plotting our new raster, we can immediately see how useful the NDVI metric is for identifying land features. Use the interactive \code{drawExtent()} function to zoom into some of the features to inspect them more closely.

<<drawextent, eval=FALSE>>=
# first, plot the raster
plot(ndvi)
# call drawExtent() to activate interactive mode
# and assign the result to an extent object e
e <- drawExtent()
# now click 2 points on the plot window
# these represent the top-right and bottom-left corner of your extent
# now plot ndvi again, but only the extent you defined interactively
plot(ndvi, ext=e)
@

\section{Classifying raster data}

One of the most important tasks in analysis of remote sensing image analysis is image classification. In classifying the image, we take the information contained in the various bands (possibly including other synthetic bands such as NDVI or principle components). In this tutorial we will explore two approaches for image classification: unsupervised (k-means) and supervised (random forest) classification.

\subsection*{Supervised classification: Random Forest\footnote{This section was written using code contributed by Valerio Avitabile.}}

The Random Forest classification algorithm is an ensemble learning method that is used for both classification and regression. In this study, we will use the algorithm to derive land cover classes given a set of training data. Random Forest builds a number of classification trees, and the final class assigned is based on the class with the maximum instances among the final set ('forest') of classes\footnote{For a more complete description of the Random Forests classification method, see \url{http://stat-www.berkeley.edu/users/breiman/RandomForests/cc\textunderscore home.htm}.
}.

One major advantage of the Random Forest method is the fact that an 'Out of the Box' (OOB) error estimate and an estimate of variable performace are performed. For each classification tree assembled, a fraction of the training data are left out and used to compute the error for each tree. In addition an importance score is computed for each variable in two forms: the mean decrease in accuracy for each variable, and the Gini impurity criterion.

We should first prepare the data on which the classification will be done. So far, we have prepared three bands from a ETM+ image in 2001 (bands 2, 3 and 4) as a rasterBrick, and have also calculated NDVI. In addition, there is a Vegetation Continuous Field (VCF) product available for the same period (2000)\footnote{For more information on the Landsat VCF product, see \url{http://glcf.umd.edu/data/landsatTreecover/}.}. This product is also based on Landsat ETM+ data, and represents an estimate of tree cover (in \%). Since this layer could also be useful in classifying land cover types, we will also include it as a potential covariate in the random forest classification.

<<vcfGewata, fig=FALSE>>=
# load the data
data(vcfGewata)
plot(vcfGewata)
histogram(vcfGewata) # or 'hist(vcfGewata)'
@

Note that in the vcfGewata rasterLayer there are some values much greater than 100, which are flags for water, cloud or cloud shadow pixels. To avoid these layers, we can assign a value of NA to these pixels so they are not used in the classification.

<<vcfGewata2, fig=FALSE>>=
vcfGewata[vcfGewata > 100] <- NA
plot(vcfGewata)
histogram(vcfGewata)
@

To perform the classification in R, it is best to assemble all covariate layers into one rasterBrick object. In this case, we can simply append these new layers (NDVI and VCF) to our existing rasterBrick (currently consisting of bands 2, 3, and 4). But first, let's rescale the NDVI layer by 10000 (just as the reflectance bands 2, 3, and 4 have been scaled) and store it as an integer raster.

<<covariants, fig=FALSE>>=
ndvi <- calc(ndvi, fun = function(x) floor(x*10000))
# change the data type
# see ?dataType for more info
dataType(ndvi) <- "INT2U"
# name this layer to make plots interpretable
names(ndvi) <- "NDVI"
# make the covariate rasterBrick
covs <- addLayer(gewata, ndvi, vcfGewata)
plot(covs)
@

For this exercise, we will do a very simple classification for 2001 using three classes: forest, cropland and wetland. While for other purposes it is usually better to define more classes (and possibly fuse classes later), a simple classification like this one could be useful, for example, to construct a forest mask for the year 2001.

<<trainingPoly, fig=FALSE, results=hide>>=
# load the training polygons
data(trainingPoly)
# inspect the data
trainingPoly@data
# superimpose training polygons onto ndvi plot
plot(ndvi)
plot(trainingPoly, add = TRUE)
@

The training classes are labelled as string labels. For this exercise, we will need to work with integer classes, so we will need to first 'relabel' our training classes. There are several approaches that could be used to convert these classes to integer codes. In this case, we will first make a function that will reclassify the character strings representing land cover classes into integers based on the existing factor levels.

<<reclassify>>=
# inspect the data slot of the trainingPoly object
trainingPoly@data
# the 'Class' column is actually an ordered factor type
trainingPoly@data$Class
str(trainingPoly@data$Class)

# define a reclassification function which substitutes
# the character label for the factor level (between 1 and 3)
reclass <- function(x){
  which(x==levels(trainingPoly@data$Class))
}

# use sapply() to apply this function over each element of the 'Class' column
# and assign to a new column called 'Code'
trainingPoly@data$Code <- sapply(trainingPoly@data$Class, FUN=reclass)
@

To train the raster data, we need to convert our training data to the same type using the \code{rasterize()} function. This function takes a spatial object (in this case a polygon object) and transfers the values to raster cells defined by a raster object. Here, we will define a new raster containing those values.

<<training_plot, fig=FALSE, results=hide>>=
# assign 'Code' values to raster cells (where they overlap)
classes <- rasterize(trainingPoly, ndvi, field='Code')
# set the dataType of the raster to INT1U
# see ?dataType for more information
dataType(classes) <- "INT1U"
# define a colour scale for the classes
# corresponding to: cropland, forest, wetland
cols <- c("orange", "dark green", "light blue")
# plot without a legend
plot(classes, col=cols, legend=FALSE)
# add a customized legend
legend("topright", legend=c("cropland", "forest", "wetland"), fill=cols, bg="white")
@

(Note: there is a handy ``\code{progress="text"}" argument, which can be passed to many of the raster package functions and can help to monitor processing. Try passing this argument to the \code{rasterize()} command above).

The legend in the plot above is obviously not ideal for these type of data (ie. values between 1.0 and 2.0 are meaningless). With \code{ggplot()} we make a more meaningful plot of factor (categorical) rasters, and reassign the original character names to the classes.

Our goal in preprocessing these data is to have a table of values representing all layers (covariates) with \emph{known} values/classes. To do this, we will first need to create a version of our rasterBrick only representing the training pixels. Here the \code{mask()} function from the raster package will be very useful.

<<masked_covariates, fig=FALSE>>=
covmasked <- mask(covs, classes)
plot(covmasked)
# add the classes layer to this new brick
names(classes) <- "class"
trainingbrick <- addLayer(covmasked, classes)
plot(trainingbrick)
# Note that in this plot, the 'class' legend is not meaningful
# This plot is useful simply to check the available layers
@

Now it's time to add all of these values to a data.frame representing all training data. This data.frame will be used as an input into the RandomForest classification function. We will use \code{getValues()} to extract all of the values from the layers of the rasterBrick.

<<valuetable, results=hide>>=
# extract all values into a matrix
valuetable <- getValues(trainingbrick)
# convert to a data.frame and inspect the first and last rows
valuetable <- as.data.frame(valuetable)
head(valuetable)
tail(valuetable)
@

In inspecting this training data.frame, you will notice that a significant number of rows has the value NA for the class column, which will be problematic during the training phase. The rows with class=NA represent pixels found outside the training polygons, and these rows should therefore be removed before going ahead with deriving the Random Forest model.

<<valuetable2, results=hide>>=
# keep only rows where valuetable$classes has a value
valuetable <- valuetable[!is.na(valuetable$class),]
head(valuetable)
tail(valuetable)

# convert values in the class column to factors
valuetable$class <- factor(valuetable$class, levels = c(1:3))
@

Now we have a convenient reference table which contains, for each of the three defined classes, all known values for all covariates. Before proceeding with the classification, let's visualize the distribution of some of these covariates using \code{ggplot()}. (Note: to make the following plots more readable, we will add a column with the class labels as characters. But we will not use this column when performing the classification.)

<<prep_ggplots, fig=FALSE>>=
# add a label column to valuetable
valuetable$label <- with(valuetable, ifelse(class==1, "cropland", 
                                            ifelse(class==2, "forest", "wetland")))
# see ?ifelse() for more information
@
<<NDVIVCFB3B4_ggplots, fig=FALSE, eval=FALSE>>=
# Now make the ggplots using valuetable$label to split the data into facets

# 1. NDVI
p1 <- ggplot(data=valuetable, aes(x=NDVI)) + 
  geom_histogram(binwidth=300) + 
  facet_wrap(~ label) +
  theme_bw()
p1

# 2. VCF
p2 <- ggplot(data=valuetable, aes(x=vcf2000Gewata)) +
  geom_histogram(binwidth=5) +
  labs(x="% Tree Cover") +
  facet_wrap(~ label) +
  theme_bw()
p2
# 4. Bands 3 and 4
p3 <- ggplot(data=valuetable, aes(x=gewataB3, y=gewataB4)) +
  stat_bin2d() +
  facet_wrap(~ label) +
  theme_bw()
p3

@
<<B2B3_ggplots, fig=TRUE, include=FALSE>>=
# 4. Bands 2 and 3
p4 <- ggplot(data = valuetable, aes(x=gewataB2, y=gewataB3)) +
  stat_bin2d() +
  facet_wrap(~ label) +
  theme_bw()
p4

@

\begin{figure}[!htp]
\centering
\includegraphics[width=\textwidth]{Lesson_6-B2B3_ggplots}
\caption{Band2-Band3 scatterplots for the three training classes.}
\label{B2B3_scatterplots}
\end{figure}

We can see from these distributions (e.g. Figure \ref{B2B3_scatterplots}) that these covariates may do well in classifying forest pixels, but we may expect some confusion between cropland and wetland (although the individual bands may help to separate these classes). When performing this classification on large datasets and with a large amount of training data, now may be a good time to save this table using the \code{write.csv()} command, in case something goes wrong after this point and you need to start over again.

Now it is time to build the Random Forest model using the training data contained in the table of values we just made. For this, we will use the "randomForest" package in R, which is an excellent resource for building such types of models. Using the \code{randomForest()} function, we will build a model based on a matrix of predictors or covariates (ie. the first 5 columns of valuetable) related to the response (the 'class' column of valuetable).

<<clean_valuetable>>=
# NA values are not permitted in the covariates/predictor columns
# keep only the rows with containing no NA's
valuetable <- na.omit(valuetable)
@
<<randomforest, eval=FALSE>>=
# construct a random forest model
# covariates (x) are found in columns 1 to 5 of valuetable
# training classes (y) are found in the 'class' column of valuetable
# caution: this step takes fairly long!
library(randomForest)
modelRF <- randomForest(x=valuetable[,c(1:5)], y=valuetable$class,
                        importance = TRUE)
@
<<RF_background, echo=FALSE, results=hide>>=
library(randomForest)
if(!file.exists("data/modelRF.rda")){
  modelRF <- randomForest(x = valuetable[,c(1:5)], y = valuetable$class,
                        importance = TRUE)
  save(modelRF, file='data/modelRF.rda', compress="bzip2", ascii=FALSE)
} else {
  load('data/modelRF.rda')
}
@

Since the random forest method involves the building and testing of many classification trees (the 'forest'), it is a computationally expensive step (and could take alot of memory for especially large training datasets). When this step is finished, it would be a good idea to save the resulting object with the \code{save()} command. Any R object can be saved as an .rda file and reloaded into future sessions.

The resulting object from the \code{randomForest()} function is a specialized object of class "randomForest", which is a large list-type object packed full of information about the model output. Elements of this object can be called and inspected like any list object.

<<inspect_RFmodel, results=hide>>=
# inspect the structure and element names of the resulting model
class(modelRF)
str(modelRF)
names(modelRF)
# inspect the confusion matrix of the OOB error assessment
modelRF$confusion
# to make the confusion matrix more readable
colnames(modelRF$confusion) <- c("cropland", "forest", "wetland", "class.error")
rownames(modelRF$confusion) <- c("cropland", "forest", "wetland")
modelRF$confusion
@

Since we set 'importance=TRUE', we now also have information on the statistical importance of each of our covariates which we can visualize using the \code{varImpPlot()} command.

<<varImpPlot, fig=TRUE, include=FALSE>>=
varImpPlot(modelRF)
@

\begin{figure}[!htp]
\centering
\includegraphics[width=0.6\textwidth]{Lesson_6-varImpPlot}
\caption{Variable importance plots for a Random Forest model showing the mean decrease in accuracy (left) and the decrease in Gini Impurity Coefficient (right) for each variable.}
\label{varimp}
\end{figure}

These two plots give two different reports on variable importance (see \code{?importance()}). First, the mean decrease in accuracy indicates the amount by which the classification accuracy decreased based on the OOB assessment. Second, the Gini impurity coefficient gives a measure of class homogeneity. More specifically, the decrease in the Gini impurity coefficient when including a particular variable is shown in the plot \footnote{From Wikipedia: "Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset". See \url{http://en.wikipedia.org/wiki/Decision_tree_learning}}.

In this case, it seems that Gewata bands 3 and 4 have the highest impact on accuracy, while bands 3 and 2 score highest with the Gini impurity criterion (Figure \ref{varimp}). For especially large datasets, it may be helpful to know this information, and leave out less important variables for subsequent runs of the \code{randomForest()} function.

Since the VCF layer included NA's (which have also been excluded in our results) and scores relatively low according to the mean accuracy decrease criterion, try to construct an alternate random forest model as above, but excluding this layer. What effect does this have on the overall accuracy of the results (hint: compare the confusion matrices of the original and new outputs). What affect does leaving this variable out have on the processing time (hint: use \code{system.time()})?

Now we can apply this model to the rest of the image and assign classes to all pixels. Note that for this step, the names of the raster layers in the input brick (here 'covs') must correspond to the column names of the training table. We will use the \code{predict()} function from the raster package to predict class values based on the random forest model we have just constructed. This function uses a pre-defined model to predict values of raster cells based on other raster layers. This model can be derived by a linear regression, for example. In our case, we will use the model provided by the \code{randomForest()} function we applied earlier.

<<predictRaster, fig=TRUE, include=FALSE, results=hide>>=
# check layer and column names
names(covs)
names(valuetable)
# predict land cover using the RF model
predLC <- predict(covs, model=modelRF, na.rm=TRUE)
# plot the results
# recall: 1 = cropland, 2 = forest, 3 = wetland
cols <- c("orange", "dark green", "light blue")
plot(predLC, col=cols, legend=FALSE)
legend("bottomright", legend=c("cropland", "forest", "wetland"), fill=cols, bg="white")
@
\begin{figure}[!htp]
\centering
\includegraphics[width=0.6\textwidth]{Lesson_6-predictRaster}
\caption{Resulting land cover map using a Random Forest classifier.}
\label{RandomForestLC}
\end{figure}

Note that the \code{predict()} function also takes arguments that can be passed to \code{writeRaster()} (eg. \code{filename = ``..."}, so it would be a good idea to write to file as you perform this step (rather than keeping all output in memory).

\subsection*{Unsupervised classification: k-means\footnote{This section will not be included in the lecture.}}

In the absence of training data, an unsupervised classification can be carried out. Unsupervised classification methods assign classes based on inherent structures in the data without resorting to training of the algorithm. One such method, the k-means method, divides data into clusters based on Euclidean distances from cluster means in a feature space\footnote{For more information on the theory behind k-means clustering, see \url{http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/kmeans.html\#macqueen}}.

We will use the same layers (from the 'covs' rasterBrick) as in the Random Forest classification for this classification exercise. As before, we need to extract all values into a data.frame.

<<kmeans_valuetable, results=hide>>=
valuetable <- getValues(covs)
head(valuetable)
@

Now we will construct a kmeans object using the \code{kmeans()} function. Like the Random Forest model, this object packages useful information about the resulting class membership. In this case, we will set the number of clusters to three, presumably corresponding to the three classes defined in our random forest classification.

<<kmeans, eval=FALSE>>=
km <- kmeans(na.omit(valuetable), centers = 3, iter.max = 100, nstart = 10)
@
<<kmeans_background, echo=FALSE>>=
if(!file.exists('data/km.rda')){
  km <- kmeans(na.omit(valuetable), centers = 3, iter.max = 100, nstart = 10)
  save(km, file='data/km.rda', compres='bzip2', ascii=FALSE)
} else {
  load('data/km.rda')
}
@
<<inspect_km, results=hide>>=
# km contains the clusters (classes) assigned to the cells
head(km$cluster)
unique(km$cluster) # displays unique values
@

As in the random forest classification, we used the \code{na.omit()} argument to avoid any NA values in the valuetable (recall that there is a region of NAs in the VCF layer). These NAs are problematic in the \code{kmeans()} function, but omitting them gives us another problem: the resulting vector of clusters (from 1 to 3) is shorter than the actual number of cells in the raster. In other words: how do we know which clusters to assign to which cells? To answer that question, we need to have a kind of 'mask' raster, indicating where the NA values throughout the cov rasterBrick are located.

<<clean_km>>=
# create a blank raster with default values of 0
rNA <- setValues(raster(covs), 0)
# loop through layers of covs
# assign a 1 to rNA wherever an NA is enountered in covs
for(i in 1:nlayers(covs)){
  rNA[is.na(covs[[i]])] <- 1
}
# convert rNA to an integer vector
rNA <- getValues(rNA)
# substitue the NA's for 0's
rNA[is.na(rNA)] <- 0
@

We now have a vector indicating with a value of 1 where the NA's in the cov brick are. Now that we know where the 'original' NAs are located, we can go ahead and assign the cluster values to a raster. At these 'NA' locations, we will not assign any of the cluster values, instead assigning an NA.

First, we will insert these values into the original valuetable data.frame.

<<assign_classes>>=
# convert valuetable to a data.frame
valuetable <- as.data.frame(valuetable)
# assign the cluster values (where rNA != 1)
valuetable$class[rNA==0] <- km$cluster
# assign NA to this column elsewhere
valuetable$class[rNA==1] <- NA
@

Now we are finally ready to assign these cluster values to a raster. This will represent our final classified raster.

<<plot_kmeans, fig=TRUE, include=FALSE, results=hide>>=
# create a blank raster
classes <- raster(covs)
# assign values from the 'class' column of valuetable
classes <- setValues(classes, valuetable$class)
plot(classes, legend=FALSE, col=c("dark green", "orange", "light blue"))
@
\begin{center}
\begin{figure}[!htp]
\centering
\includegraphics[width=0.6\textwidth]{Lesson-6_plot_kmeans}
\caption{Unsupervised land cover classification resulting from k-means method. At this stage, the classes are undefined, so the colours are somewhat arbitrary.}
\label{kmeansLC}
\end{figure}
\end{center}

These classes are much more difficult to interpret than those resulting from the random forest classification. We can see from Figure \ref{kmeansLC} that there is particularly high confusion between (what we might assume to be) the cropland and wetland classes. Clearly, with a good training dataset, a supervised classification can provide a reasonably accurate land cover classification. However, unsupervised classification methods like k-means are useful for study areas for which little to no \emph{a priori} data exist. Assuming there are no training data available, is there a way we could improve the k-means classification performed in this example? Which one is computationally faster between random forest and k-means (hint: try the \code{system.time()} function)?

\subsection*{Applying a raster sieve}

Although the land cover raster we created with the Random Forest method above is limited in the number of thematic classes it has, and we observed some confusion between wetland and cropland classes, it could be useful for constructing a forest mask. To do so, we have to fuse (and remove) non-forest classes, and then clean up the remaining pixels using focal algebra to apply a sieve.

<<formask, fig=FALSE>>=
# Make an NA-value raster based on the LC raster attributes
formask <- setValues(raster(predLC), NA)
# assign 1 to all cells corresponding to the forest class
formask[predLC==2] <- 1
plot(formask, col="dark green", legend = FALSE)
@

\begin{figure}[!htp]
  \centering
  \includegraphics[width=0.6\textwidth]{figs/rasterSieves.png}
  \caption{Application of a sieve on a forest mask (left top and bottom). The first sieve (centre) defines 'neighbour' pixels as being either adjacent or diagonal. The second sieve (right) defines 'neighbour' pixels as only those lying adjacent to the centre pixel. In the first sieve, the top example results in an NA being assigned to the centre pixel, and the original value being kept in the bottom example. In the second sieve, both examples result in an NA being assigned to the centre pixel.}
  \label{sieveExample}
\end{figure}

We now have a forest mask that can be used to isolate forest pixels for further analysis. For some applications, however, we may only be interested in larger forest areas. We may especially want to remove single forest pixels, as they may be a result of errors, or may not fit our definition of 'forest'. In this section, we will construct 2 types of sieves to remove these types of pixels, following 2 definitions of 'island' pixels: (1) pixels with absolutely no neighbours; and (2) pixels with no adjacent neighbours (ie. ignoring diagonal neighbours). A demonstration is shown in Figure \ref{sieveExample}.

To remove these pixel 'islands', we will use the \code{focal()} function of the raster package, which computes values based on values of neighbouring pixels. The first task is to construct a mask with the \code{focal()} function, which will be used to 'clean' up the forest mask we have just produced.

<<sievemask, results=hide, fig=FALSE>>=
# sum of all neighbourhood pixels in formask
sievemask <- focal(formask, w=3, fun=sum, na.rm=TRUE)
sievemask
histogram(sievemask)
@

We now have a mask whose values are the sum of neighbourhood weights (each equal to 1/9, based on a 3X3 window). To make this more comparable with the 2nd sieve (ignoring diagonals), we should multiply these values by 9 (resulting in whole numbers). 

<<sievemask_normalize, results=hide, fig=FALSE>>=
sievemask <- sievemask * 9
histogram(sievemask)
@

Now, in cases where there are no neighbours (ie. the 'island' pixels), this sum will be equal to 1 exactly. In cases with one or more neighbours, the sum will be greater than 1, up to a maximum of 9 (ie. if the pixel is completely surrounded by non-NA pixels). To apply the sieve and remove 'island' pixels, we want to select for only cases where sievemask == 1 and remove those from the original raster.

<<final_sieve, fig=TRUE, include=FALSE>>=
# copy the original forest mask
formaskSieve <- formask
# assign NA to pixels where the sievemask == 1
formaskSieve[sievemask==1] <- NA
# zoom in to a small extent to check the results
# Note: you can define your own zoom by using e <- drawExtent()
e <- extent(c(811744.8, 812764.3, 849997.8, 850920.3))
opar <- par(mfrow=c(1, 2)) # allow 2 plots side-by-side
plot(formask, ext=e, col="dark green", legend=FALSE)
plot(formaskSieve, ext=e, col="dark green", legend=FALSE)
par(opar) # reset plotting window
@
\begin{center}
\begin{figure}[!htp]
\centering
\includegraphics[width=0.6\textwidth]{Lesson_6-final_sieve}
\caption{Zoom of a forest mask before (left) and after (right) application of a sieve.}
\label{sieve}
\end{figure}
\end{center}

We have successfully removed all 'island' pixels from the forest mask using the \code{focal()} function. Suppose we define 'island' pixels as those have no immediate neighbours, \emph{not} considering diagonal neighbours. In that case, we would have to adjust the weight argument ('w') in \code{focal()}, and instead define our own 3X3 matrix which omits diagnoal pixels by assigning a zero-weight to these pixels.

<<sievemask_nodiag, results=hide>>=
# define a weights matrix
w <- rbind(c(0, 1, 0),
           c(1, 1, 1),
           c(0, 1, 0))
print(w)
# alternatively:
w <- matrix(c(0, 1, 0, 1, 1, 1, 0, 1, 0), nrow=3)

# sum of all neighbouring pixels in formask, except for diagonals
sievemask <- focal(formask, w=w, fun=sum, na.rm=TRUE)
@

Applying the new sieve as above should give the following results shown in \ref{sieveDiagonal}.

<<final_sieve_nodiag, fig=TRUE, include=FALSE, echo=FALSE>>=
# copy the original forest mask
formaskSieve <- formask
# assign NA to pixels where the sievemask == 1/9
formaskSieve[sievemask==1] <- NA
e <- extent(c(811744.8, 812764.3, 849997.8, 850920.3))
opar <- par(mfrow = c(1, 2))
plot(formask, ext=e, col="dark green", legend=FALSE)
plot(formaskSieve, ext=e, col="dark green", legend=FALSE)
par(opar)
@

\begin{center}
\begin{figure}[!htp]
\centering
\includegraphics[width=0.6\textwidth]{Lesson_6-final_sieve_nodiag}
\caption{Zoom of a forest mask before (left) and after (right) application of a sieve, without consideration for diagonal neighbours.}
\label{sieveDiagonal}
\end{figure}
\end{center}

In this case, not only pixels with absolutely no neighbours have been removed, but also those pixels with only diagonal neighbours as well. The second case could be considered as a more 'conservative' sieve.

The \code{focal()} function can be used for a variety of purposes, including other data filters such as median, Laplacian, Sobel, etc. More complex weight matrices can also be computed, such as a Gaussian using the \code{focalWeight()} function. These methods will not be covered in this tutorial. Check out the documentation at \code{?focal()} for more information.

\subsection*{Working with thematic rasters}

As we have seen with the land cover rasters we derived using the random forest or k-means methods above, the values of a raster may be categorical, meaning they relate to a thematic class (e.g. 'forest' or 'wetland') rather than a quantitative value (e.g. NDVI or \% Tree Cover). The raster dataset 'lulcGewata' is a raster with integer values representing Land Use and Land Cover (LULC) classes from a 2011 classification (using SPOT5 and ASTER source data).

<<lulcGewata>>=
data(lulcGewata)
# check out the distribution of the values
freq(lulcGewata)
hist(lulcGewata)
@

This is a raster with integer values between 1 and 6, but for this raster to be meaningful at all, we need a lookup or attribute table to identify these classes. A data.frame defining these classes is also included in the rasta package:

<<LUTGewata>>=
data(LUTGewata)
LUTGewata
@

This data.frame represents a lookup table for the raster we just loaded. The ID column corresponds to the values taken on by the lulc raster, and the 'Class' column describes the LULC classes assigned. In R it is possible to add a attribute table to a raster. In order to do this, we need to coerce the raster values to a factor from an integer.

<<lulc_factor>>=
lulc <- as.factor(lulcGewata)
@

If you display the attributes of this raster (just type 'lulc'), it will do so, but will also return an error. This error arises because R expects that a raster with factor values should also have a raster attribute table.

<<lulc_RAT, results=hide>>=
# assign a raster attribute table (RAT)
levels(lulc) <- LUTGewata
lulc
@

In some cases it might be more useulf to visualize only one class at a time. The \code{layerize()} function in the raster package does this by producing a rasterBrick object with each layer representing the class membership of each class as a boolean.

<<layerize, fig=FALSE, results=hide>>=
classes <- layerize(lulc)
plot(classes)
# layer names follow the order of classes in the LUT
names(classes) <- LUTGewata$Class
plot(classes, legend=FALSE)
@

Now each class is represented by a separate layer representing class membership of each pixel with 0's and 1's. If we want to construct a forest mask as we did above, this is easily done by extracting the fifth layer of this rasterBrick and replacing 0's with NA's.

<<layerize_formask, results=hide>>=
forest <- raster(classes, 5)
# is equivalent to:
forest <- classes[[5]]
# or (since the layers are named):
forest <- classes$forest
# replace 0's (non-forest) with NA's
forest[forest==0] <- NA
plot(forest, col="dark green", legend=FALSE)
@

\section{Exercise}

Perform another Random Forest classification with 6 classes (found in the lulcGewata raster dataset) using all (or some) of the bands used in this tutorial (b2, b3, b4, ndvi and vcf). This time, derive your own training data from the lulcGewata image provided with this package. This can be done interactively by creating polygons using the \code{drawExtent()} command

<<exercise_guide, eval=FALSE, echo=FALSE>>=
# load in the training dataset and LUT
data(lulcGewata)
data(LUTGewata)

# plot lulcGewata with a meaningful legend (see LUTGewata)
# make sure the classes correspond correctly!
cols <- c("orange", "light green", "brown", "light pink", "dark green", "light blue")
plot(lulcGewata, col=cols, legend=FALSE)
legend("topright", legend=LUTGewata$Class, fill=cols)

# draw an Spatialpolygons object in area purely represented by cropland
# so, first plot the raster again
plot(lulcGewata, col=cols, legend=FALSE)
cropland <- drawPoly(sp=TRUE)
# click on "Finish" in the top-right corner (if using Rstudio) when finished
# this outputs an sp (SpatialPolygons) object with 1 row (feature)
# if you want more training data for a particular class:
# append another polygon onto the same object using gUnion() of the rgeos package
cropland <- gUnion(cropland, drawPoly(sp=TRUE))

# when you are finished with this class, be sure to set the coordinate reference system (CRS)
projection(cropland) <- projection(lulcGewata)
# and check
projection(cropland)
plot(lulcGewata); plot(cropland, add=TRUE)

# convert it to a SpatialPolygonsDataFrame (ie. add a @data slot)
cropland <- SpatialPolygonsDataFrame(cropland, data=data.frame(class="cropland"), match.ID=FALSE)

# do the same for the other 5 classes...
# you may have to zoom in first for (e.g.) coffee investment areas
plot(lulcGewata, col=cols, legend=FALSE)
e <- drawExtent() # zoom into a coffee area
plot(lulcGewata, col=cols, legend=FALSE, ext=e)
# now define a training polygon
coffee <- drawPoly(sp=TRUE)
projection(coffee) <- projection(lulcGewata)
plot(lulcGewata); plot(coffee, add=TRUE)
coffee <- SpatialPolygonsDataFrame(coffee, data=data.frame(class="coffee investment area"), match.ID=FALSE)

# once all polygons have been drawn, fusing them into one SpatialPolygons object is problematic
# because the Polygon ID's are not unique
# use spChFIDs() from the sp package to solve this
# e.g. for the cropland features
cropland <- spChFIDs(cropland, "cropland")
forest <- spChFIDs(forest, "forest")
coffee <- spChFIDs(coffee, "coffee")
# etc...

# now they can be bound (2 at a time) as one object using spRbind (maptools)
trainingPoly <- spRbind(cropland, forest)
trainingPoly <- spRbind(trainingPoly, coffee)
# etc...
# check
trainingPoly@data
plot(lulcGewata); plot(trainingPoly, add=TRUE)

### now proceed with the Random Forest classification using the Gewata covariates as shown above

@

Your report on the results and accompanying code should include the following:
\begin{itemize}
\item a plot of the original lulcGewata raster with a meaningful legend (ie. classes as characters (strings)!)
\item a plot of your training polygons
\item the resulting thematic map with meaningful legend (as above)
\item the output OOB confusion matrix with accuracy per class
\item the variable importance ranks (mean accuracy decrease and mean Gini coefficient decrease)
\item your own confusion matrix derived from an overlay of the original lulcGewata raster with your own LULC raster and resulting accuracy statistics.
\end{itemize}

In this exercise, also address the following questions:
\begin{enumerate}
\item Which classes have the highest accuracy? Lowest?
\item Is the importance ranking of the input bands different in this case to the 3-class classification we did earlier? If so, how, and what has changed in this case?
\item Can you say something about class separability? Hint: see \code{?zonal()} and try to derive mean and standard deviation of several of the input bands for each class. How do these values compare with each other? Are there any classes which show high overlap? Is this consistent with the confusion matrices you derived earlier?
\end{enumerate}

% \bibliographystyle{model5-names}
% \bibliography{refs}

\end{document}
